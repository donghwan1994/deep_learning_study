{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pylab as plt\n",
    "from common.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE(Mean Squared Error)\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEE(Cross Entropy Error)\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch size 10, random choice\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "X_batch = X_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9709, 48932,  9616, 36518, 12363, 46188, 12049,  8088,  1679,\n",
       "       46635])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(train_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cross_entropy_error(y, t):\n",
    "#    if y.ndim == 1:\n",
    "#        t = t.reshape(1, t.size)\n",
    "#        y = y.reshape(1, y.size)\n",
    "#        \n",
    "#    batch_size = y.shape[0]\n",
    "#    return -np.sum(t*np.log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_dif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-56bc1dbdd435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnumerical_dif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'numerical_dif' is not defined"
     ]
    }
   ],
   "source": [
    "numerical_dif(function_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2) # or return x[0]**2 + x[1]**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "gradient_descent(function_2, init_x = init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26043879e+00  2.56292196e-01 -2.70182404e-01]\n",
      " [-1.13370845e+00  2.53981380e-03  2.66204181e+00]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26407433  0.15606115  2.23372819]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # 최대값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6388975600289095"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.07736682 -0.41333173  1.49069855]\n",
      " [-1.61605023 -0.6199976   2.23604783]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return  y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10000 # interate num\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # mini batch size\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # get mini batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # calculate gradient\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # updata parameter\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # Record train progress\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.294670995669584,\n",
       " 2.2939784518373356,\n",
       " 2.275984474882529,\n",
       " 2.2950693680032224,\n",
       " 2.2714083673979193,\n",
       " 2.305289469948913,\n",
       " 2.2824848235079913,\n",
       " 2.308658422814168,\n",
       " 2.289774186271946,\n",
       " 2.291711337936973,\n",
       " 2.2862849741602687,\n",
       " 2.2796609292062966,\n",
       " 2.2862791835402025,\n",
       " 2.285475885687771,\n",
       " 2.301428611558028,\n",
       " 2.269631808006825,\n",
       " 2.290246283554448,\n",
       " 2.292661833132514,\n",
       " 2.305474544724655,\n",
       " 2.2735541668236987,\n",
       " 2.3040399812109142,\n",
       " 2.2984735957911253,\n",
       " 2.287248001381914,\n",
       " 2.2945959720151956,\n",
       " 2.2946003919005515,\n",
       " 2.2860909944738115,\n",
       " 2.3006048781702626,\n",
       " 2.2806564119757833,\n",
       " 2.28424913483253,\n",
       " 2.30435582510569,\n",
       " 2.2787854285299347,\n",
       " 2.279777256622986,\n",
       " 2.28891745753227,\n",
       " 2.293719852814004,\n",
       " 2.2831475722079038,\n",
       " 2.272643283985849,\n",
       " 2.2917151040175825,\n",
       " 2.277212085079249,\n",
       " 2.29882559315961,\n",
       " 2.297073014095561,\n",
       " 2.2839018564717883,\n",
       " 2.3027203329523913,\n",
       " 2.287811386925867,\n",
       " 2.286668338677895,\n",
       " 2.2745353475643015,\n",
       " 2.3058201240484975,\n",
       " 2.304768286757148,\n",
       " 2.2927986151497377,\n",
       " 2.2916633037670344,\n",
       " 2.2927439405850176,\n",
       " 2.284846569341487,\n",
       " 2.2793686524439902,\n",
       " 2.2949387792420386,\n",
       " 2.3044185584005397,\n",
       " 2.287918245260054,\n",
       " 2.2804521349978155,\n",
       " 2.294364508347561,\n",
       " 2.2814166766855535,\n",
       " 2.2940463937860396,\n",
       " 2.285999918625997,\n",
       " 2.27163282724844,\n",
       " 2.2859253427700175,\n",
       " 2.2838584936842237,\n",
       " 2.2799173299854893,\n",
       " 2.2911267942455735,\n",
       " 2.27323789171527,\n",
       " 2.289334725267329,\n",
       " 2.2867646507959294,\n",
       " 2.289940452217632,\n",
       " 2.2927546449438747,\n",
       " 2.282380821135919,\n",
       " 2.2995713209574187,\n",
       " 2.2841245915352375,\n",
       " 2.2798148922726234,\n",
       " 2.256384670340872,\n",
       " 2.2842734685306794,\n",
       " 2.283107226914825,\n",
       " 2.2885447773331222,\n",
       " 2.2923520425815584,\n",
       " 2.257173827053211,\n",
       " 2.281538153374484,\n",
       " 2.2933726344835517,\n",
       " 2.280604090346779,\n",
       " 2.274274090941074,\n",
       " 2.2488718609455565,\n",
       " 2.273739857654275,\n",
       " 2.303474377915667,\n",
       " 2.2916137747303047,\n",
       " 2.2834797497268435,\n",
       " 2.266072108943365,\n",
       " 2.2733125693193217,\n",
       " 2.266562859930777,\n",
       " 2.2584029517445665,\n",
       " 2.291022770000022,\n",
       " 2.2893591623432523,\n",
       " 2.291255744676412,\n",
       " 2.2650155026249243,\n",
       " 2.273072317727569,\n",
       " 2.278623071998075,\n",
       " 2.2801999536240958,\n",
       " 2.2913009560416047,\n",
       " 2.282841352695668,\n",
       " 2.2760346470703032,\n",
       " 2.275593255170548,\n",
       " 2.2614517960573957,\n",
       " 2.295762668352074,\n",
       " 2.279589855256319,\n",
       " 2.281567889411268,\n",
       " 2.282402780995711,\n",
       " 2.2682583953187367,\n",
       " 2.2633667968886453,\n",
       " 2.2830013851026862,\n",
       " 2.2758501656025127,\n",
       " 2.273876566562083,\n",
       " 2.2684071172804745,\n",
       " 2.2810236879075863,\n",
       " 2.2681977862420086,\n",
       " 2.2861879921777626,\n",
       " 2.266963212982786,\n",
       " 2.2610975174326784,\n",
       " 2.2627570747377344,\n",
       " 2.2767120237614313,\n",
       " 2.2647134973256677,\n",
       " 2.2879434360638946,\n",
       " 2.267267814074253,\n",
       " 2.278042310438431,\n",
       " 2.2627690228743402,\n",
       " 2.2589151220403143,\n",
       " 2.262032533863655,\n",
       " 2.247468299128855,\n",
       " 2.260595385260528,\n",
       " 2.275860434748912,\n",
       " 2.2624515599104673,\n",
       " 2.2524351615527496,\n",
       " 2.2496461937208587,\n",
       " 2.263291713043321,\n",
       " 2.263274651904272,\n",
       " 2.264749906157054,\n",
       " 2.2599307965708313,\n",
       " 2.2700026504170756,\n",
       " 2.247897222383787,\n",
       " 2.2512253479652036,\n",
       " 2.253928968358137,\n",
       " 2.2628646816169153,\n",
       " 2.2474031247172324,\n",
       " 2.248171048543051,\n",
       " 2.258752533060736,\n",
       " 2.2627305121230434,\n",
       " 2.2472970847507066,\n",
       " 2.2490125694704197,\n",
       " 2.251041550939688,\n",
       " 2.2461376624908813,\n",
       " 2.255155481542127,\n",
       " 2.238204221923573,\n",
       " 2.246306268863311,\n",
       " 2.2424165550805153,\n",
       " 2.2441488536985705,\n",
       " 2.238427943770221,\n",
       " 2.2376103848057594,\n",
       " 2.238644471645908,\n",
       " 2.248676796393111,\n",
       " 2.2407693130894044,\n",
       " 2.2435165768992973,\n",
       " 2.2304189502613196,\n",
       " 2.224797469270304,\n",
       " 2.239455103346247,\n",
       " 2.2329879160692,\n",
       " 2.2310301130933494,\n",
       " 2.223039344705614,\n",
       " 2.2307667417207155,\n",
       " 2.1994636982585916,\n",
       " 2.222487113855308,\n",
       " 2.2167875733008033,\n",
       " 2.2098570663936767,\n",
       " 2.223540692814167,\n",
       " 2.2170975522173064,\n",
       " 2.180852017969731,\n",
       " 2.174615792037062,\n",
       " 2.1907559215826824,\n",
       " 2.1948747032955738,\n",
       " 2.2151537694311756,\n",
       " 2.203244161564677,\n",
       " 2.204181958945448,\n",
       " 2.1975914501582587,\n",
       " 2.1916144817591414,\n",
       " 2.1736835424071574,\n",
       " 2.1858898078041182,\n",
       " 2.1840905109813917,\n",
       " 2.1802346276314255,\n",
       " 2.1630655692481744,\n",
       " 2.199291150502779,\n",
       " 2.148446242891147,\n",
       " 2.1793556487707395,\n",
       " 2.1805650347801695,\n",
       " 2.1537505020190717,\n",
       " 2.167273764158795,\n",
       " 2.1845256997206715,\n",
       " 2.17018683105505,\n",
       " 2.1594749574647754,\n",
       " 2.1810055289562795,\n",
       " 2.151973263200971,\n",
       " 2.1547273465448895,\n",
       " 2.1492760036804883,\n",
       " 2.1307389164350616,\n",
       " 2.158047165040665,\n",
       " 2.1560367749919314,\n",
       " 2.162789040048834,\n",
       " 2.1446797813364378,\n",
       " 2.1322272977803634,\n",
       " 2.126037124743692,\n",
       " 2.127802173043487,\n",
       " 2.135459716467373,\n",
       " 2.1090345095570986,\n",
       " 2.1250891247186052,\n",
       " 2.12136457906318,\n",
       " 2.0974560867465932,\n",
       " 2.0957323027439534,\n",
       " 2.113943717384956,\n",
       " 2.117584773483617,\n",
       " 2.0950285244660574,\n",
       " 2.087653401150185,\n",
       " 2.129061649582728,\n",
       " 2.089973243181209,\n",
       " 2.09681287355612,\n",
       " 2.072532709975793,\n",
       " 2.062532710330179,\n",
       " 2.0896548989322907,\n",
       " 2.0752638515466484,\n",
       " 2.0671798093408875,\n",
       " 2.067980376628037,\n",
       " 2.0932838645080376,\n",
       " 2.0377501250621295,\n",
       " 2.0526855351586373,\n",
       " 2.0283364154414527,\n",
       " 2.019240006054944,\n",
       " 2.0777842515331213,\n",
       " 2.0170618636549973,\n",
       " 2.042959217154915,\n",
       " 2.035032410050253,\n",
       " 2.031673261653282,\n",
       " 2.0114644088913867,\n",
       " 2.0290765997984623,\n",
       " 1.962690288020708,\n",
       " 2.038676494628466,\n",
       " 2.005185602614612,\n",
       " 1.9813406381191907,\n",
       " 1.963295586305178,\n",
       " 2.0032921059209614,\n",
       " 1.9937536629396009,\n",
       " 2.003961584494531,\n",
       " 1.9610429657689072,\n",
       " 1.986298610102688,\n",
       " 1.9636476750804268,\n",
       " 1.9348763241729572,\n",
       " 1.986592502780199,\n",
       " 1.9429046609777283,\n",
       " 2.0015118753330046,\n",
       " 1.9538511534608773,\n",
       " 1.9877388507153047,\n",
       " 1.950334391428745,\n",
       " 1.9567246537664307,\n",
       " 1.9035404685896935,\n",
       " 1.8669328181206308,\n",
       " 1.9728660169971226,\n",
       " 1.8801109132031255,\n",
       " 1.8565822059667212,\n",
       " 1.872721994399559,\n",
       " 1.8586693486290933,\n",
       " 1.89119713207836,\n",
       " 1.8600241646575695,\n",
       " 1.8160001617575856,\n",
       " 1.9213425103471857,\n",
       " 1.8637905631424536,\n",
       " 1.8423400172404272,\n",
       " 1.871188764795273,\n",
       " 1.8318962133154282,\n",
       " 1.8583712475922252,\n",
       " 1.8253680829406091,\n",
       " 1.8420203838684666,\n",
       " 1.821342994585766,\n",
       " 1.854413513503137,\n",
       " 1.882079201669962,\n",
       " 1.827260854783955,\n",
       " 1.7919725275019882,\n",
       " 1.8306888910113381,\n",
       " 1.8087600732663542,\n",
       " 1.753497904333775,\n",
       " 1.8925665151830129,\n",
       " 1.8817106661743628,\n",
       " 1.7345214711937325,\n",
       " 1.7009768010653263,\n",
       " 1.761349905649257,\n",
       " 1.7866546229986395,\n",
       " 1.8303529699138397,\n",
       " 1.7367035321817061,\n",
       " 1.7945563432526206,\n",
       " 1.745629648904149,\n",
       " 1.7404599129021845,\n",
       " 1.7622840743891792,\n",
       " 1.6477632477311177,\n",
       " 1.7512191163039246,\n",
       " 1.7095996048043447,\n",
       " 1.7163456833333803,\n",
       " 1.7209684479996534,\n",
       " 1.6724166691085804,\n",
       " 1.6897090172446108,\n",
       " 1.6802055047682631,\n",
       " 1.6836162505136973,\n",
       " 1.7574074901174817,\n",
       " 1.6392397114078283,\n",
       " 1.7758239625716066,\n",
       " 1.6484566913001202,\n",
       " 1.6923025830903404,\n",
       " 1.6710884488306539,\n",
       " 1.616834299794397,\n",
       " 1.6258694763960497,\n",
       " 1.6194942544532882,\n",
       " 1.625884406231102,\n",
       " 1.607383013075003,\n",
       " 1.606840309860529,\n",
       " 1.5892113918640134,\n",
       " 1.6698155965673969,\n",
       " 1.5461652354781876,\n",
       " 1.6201156544077855,\n",
       " 1.6912122300674541,\n",
       " 1.5509357988365928,\n",
       " 1.6691030192694856,\n",
       " 1.5690642616172783,\n",
       " 1.5960849625133215,\n",
       " 1.5803714112903355,\n",
       " 1.606501878946242,\n",
       " 1.5394621618457092,\n",
       " 1.4839768136444293,\n",
       " 1.6131189482084698,\n",
       " 1.4000067644173544,\n",
       " 1.4901123265241731,\n",
       " 1.5495909252100504,\n",
       " 1.5762045310858839,\n",
       " 1.4333137872238684,\n",
       " 1.5715948467077583,\n",
       " 1.4480345996063182,\n",
       " 1.682991040985156,\n",
       " 1.5146274897565795,\n",
       " 1.500559377076436,\n",
       " 1.5053078518897658,\n",
       " 1.602826247194236,\n",
       " 1.5756507379463633,\n",
       " 1.4837019926834378,\n",
       " 1.5301856642983167,\n",
       " 1.544197101751987,\n",
       " 1.4114193432286937,\n",
       " 1.4450216182294224,\n",
       " 1.4831008848313436,\n",
       " 1.4502351461678202,\n",
       " 1.4588679368760007,\n",
       " 1.382145651308489,\n",
       " 1.471630368604787,\n",
       " 1.4364246490909052,\n",
       " 1.4632283668556871,\n",
       " 1.4535231583118722,\n",
       " 1.3997790806906938,\n",
       " 1.5596652244401792,\n",
       " 1.4098304977330116,\n",
       " 1.5055608313092068,\n",
       " 1.439934698200038,\n",
       " 1.4177787349713327,\n",
       " 1.411742635061954,\n",
       " 1.4858576221095479,\n",
       " 1.3982571045697452,\n",
       " 1.394769199910839,\n",
       " 1.443010800787523,\n",
       " 1.4067165927557963,\n",
       " 1.424848545010366,\n",
       " 1.4157587876848896,\n",
       " 1.3947000248189374,\n",
       " 1.3961504782336869,\n",
       " 1.3740637694767261,\n",
       " 1.387667133298112,\n",
       " 1.339121377093549,\n",
       " 1.3523719927319746,\n",
       " 1.307182535000556,\n",
       " 1.3518118783150803,\n",
       " 1.3929760694583198,\n",
       " 1.3632061635733705,\n",
       " 1.2740239946055527,\n",
       " 1.3189175452920474,\n",
       " 1.4726323017982808,\n",
       " 1.3545955772195206,\n",
       " 1.2417020297965267,\n",
       " 1.4657290080839152,\n",
       " 1.2531809112724848,\n",
       " 1.3404295080757709,\n",
       " 1.2565111644760907,\n",
       " 1.179807931496412,\n",
       " 1.3344322962820951,\n",
       " 1.390615781130393,\n",
       " 1.1765692277493394,\n",
       " 1.2759396529588545,\n",
       " 1.2886669331164464,\n",
       " 1.2736075210152757,\n",
       " 1.2230933017644627,\n",
       " 1.3227396329931183,\n",
       " 1.2618712475394986,\n",
       " 1.3015099039400166,\n",
       " 1.2674110977102828,\n",
       " 1.322922217228772,\n",
       " 1.2334476823049272,\n",
       " 1.2401918050660392,\n",
       " 1.2967021236061376,\n",
       " 1.2332025509242657,\n",
       " 1.164165361781006,\n",
       " 1.1214929975008654,\n",
       " 1.1011299874675244,\n",
       " 1.227783219345391,\n",
       " 1.2177138521189814,\n",
       " 1.2845198385229963,\n",
       " 1.2885249522943945,\n",
       " 1.2467096511372164,\n",
       " 1.1767187313218987,\n",
       " 1.2355088802387324,\n",
       " 1.3342584967232856,\n",
       " 1.1459243238948693,\n",
       " 1.292310673284946,\n",
       " 1.25987376595033,\n",
       " 1.2207613719340338,\n",
       " 1.2366420041084105,\n",
       " 1.1313493559119392,\n",
       " 1.2090688050356335,\n",
       " 1.2100927763079512,\n",
       " 1.261705186893118,\n",
       " 1.2009771967235794,\n",
       " 1.2047486236333231,\n",
       " 1.2272877562188333,\n",
       " 1.3126018664555081,\n",
       " 1.1323924223831359,\n",
       " 1.1936994890541284,\n",
       " 1.2589051430207159,\n",
       " 1.151940706179076,\n",
       " 1.169435255621022,\n",
       " 1.1216008795012133,\n",
       " 1.1390464400371407,\n",
       " 1.07359594481049,\n",
       " 1.1449192066887435,\n",
       " 1.020569415174023,\n",
       " 1.2849881509180618,\n",
       " 1.1636134089587822,\n",
       " 1.1069530030890697,\n",
       " 1.101817920122171,\n",
       " 1.172352313650868,\n",
       " 1.0837577980025865,\n",
       " 1.0426071289452568,\n",
       " 1.1802363800605857,\n",
       " 1.1613684091065184,\n",
       " 1.123606158350447,\n",
       " 1.165403250554819,\n",
       " 1.2792061126473697,\n",
       " 1.0533741976118702,\n",
       " 1.0288126496975298,\n",
       " 1.157639432506836,\n",
       " 1.2104684642831587,\n",
       " 1.1547967556659484,\n",
       " 1.0336665582762097,\n",
       " 1.0922934171904894,\n",
       " 1.1148884741588494,\n",
       " 1.155135901252871,\n",
       " 1.04386992670909,\n",
       " 1.0666358042934332,\n",
       " 1.0963125739993287,\n",
       " 1.0982960771749732,\n",
       " 1.0356677183741714,\n",
       " 0.9551944155530133,\n",
       " 1.0776184613708581,\n",
       " 1.1160629330595329,\n",
       " 1.1334810452088755,\n",
       " 1.0880523238655195,\n",
       " 1.0613876593543579,\n",
       " 1.0419363751028692,\n",
       " 1.1030418870202643,\n",
       " 0.9682033531479652,\n",
       " 1.0483340871305777,\n",
       " 1.0346176630774506,\n",
       " 1.1077343407064126,\n",
       " 1.1105114168136805,\n",
       " 1.0120155220779663,\n",
       " 1.2205460059184483,\n",
       " 1.069969947170625,\n",
       " 1.0626729837147908,\n",
       " 1.0848248516982215,\n",
       " 1.0149278280093508,\n",
       " 1.0470082019289615,\n",
       " 1.094034474402533,\n",
       " 0.9818809461721948,\n",
       " 1.0257671545225688,\n",
       " 1.0478844608783173,\n",
       " 1.039267758849995,\n",
       " 1.0155462526024313,\n",
       " 0.9657962292722511,\n",
       " 0.9126803260929411,\n",
       " 0.8915288973975577,\n",
       " 1.1112685351158502,\n",
       " 1.0502741640212379,\n",
       " 0.9742589968816433,\n",
       " 0.9859699061941463,\n",
       " 1.0865803146939816,\n",
       " 1.0146489119414388,\n",
       " 0.9884409852743467,\n",
       " 0.9906305552060539,\n",
       " 1.0119352339273082,\n",
       " 1.0880875726132901,\n",
       " 0.9061615221060088,\n",
       " 0.9665908694878711,\n",
       " 1.0253561783265241,\n",
       " 1.0627346349812714,\n",
       " 0.9601533728863242,\n",
       " 0.905412881745436,\n",
       " 0.9516311145900782,\n",
       " 0.9694134580030125,\n",
       " 1.0636713694473072,\n",
       " 1.0499877065835646,\n",
       " 0.9283457496611344,\n",
       " 1.0838876816174396,\n",
       " 1.0081428071126166,\n",
       " 1.0788526844822828,\n",
       " 0.9064856050814707,\n",
       " 1.031833420671967,\n",
       " 0.9223506067514167,\n",
       " 1.022389712702268,\n",
       " 1.037670676192116,\n",
       " 0.9901253164807363,\n",
       " 0.9355220936556329,\n",
       " 0.8430368237425502,\n",
       " 0.9853193233305814,\n",
       " 1.1044821945114045,\n",
       " 0.9718114944602155,\n",
       " 1.0066871916630837,\n",
       " 0.9214160123181142,\n",
       " 1.0295107524164608,\n",
       " 0.9323221777217667,\n",
       " 1.006416258098058,\n",
       " 0.9506887119382756,\n",
       " 0.981301120783196,\n",
       " 0.9723945997166862,\n",
       " 0.9506771730384446,\n",
       " 0.9940973072576029,\n",
       " 0.9151163094224732,\n",
       " 0.9617215213752602,\n",
       " 0.9457814273196589,\n",
       " 0.9994577407895885,\n",
       " 0.9577996153026889,\n",
       " 0.8189487697635042,\n",
       " 0.8982705311319015,\n",
       " 0.9452566758528547,\n",
       " 0.955094649898377,\n",
       " 0.8682495338929835,\n",
       " 0.8669932671346265,\n",
       " 0.8656647290078994,\n",
       " 0.8745913277353357,\n",
       " 1.0697267528463497,\n",
       " 0.9189010977775642,\n",
       " 0.9310461073922336,\n",
       " 0.8961918282980027,\n",
       " 0.8959377560908619,\n",
       " 0.9521664184903063,\n",
       " 1.000253589679051,\n",
       " 0.9956097144846917,\n",
       " 0.9915676392526299,\n",
       " 0.903992389179065,\n",
       " 0.8158589867670385,\n",
       " 0.8588171392990686,\n",
       " 0.8351552993574173,\n",
       " 0.8541811411597798,\n",
       " 0.8647368308546615,\n",
       " 0.8807131388252164,\n",
       " 0.8943645611329498,\n",
       " 0.890171067862699,\n",
       " 0.9016397837162987,\n",
       " 0.7484632667467039,\n",
       " 0.8944929071708866,\n",
       " 0.9152682121781869,\n",
       " 0.8296485786060728,\n",
       " 0.9436087686647577,\n",
       " 0.9472372140391947,\n",
       " 0.8838139919106297,\n",
       " 0.8095260357649968,\n",
       " 0.8082825707784196,\n",
       " 0.9266875595371675,\n",
       " 0.8097514091020339,\n",
       " 0.8077738123774557,\n",
       " 0.8713148866528205,\n",
       " 0.8400859558680406,\n",
       " 0.8699047994018408,\n",
       " 0.8652151907805369,\n",
       " 0.8073872324764829,\n",
       " 0.8930920025824491,\n",
       " 0.8311571004945361,\n",
       " 0.9194920075223695,\n",
       " 0.8888059170465888,\n",
       " 0.9236434586582515,\n",
       " 0.886816648543222,\n",
       " 0.8548265654199634,\n",
       " 0.8177786439519552,\n",
       " 0.9077146923637928,\n",
       " 0.8965166549255121,\n",
       " 0.9271774355284947,\n",
       " 0.8478844779730836,\n",
       " 0.9479917044777555,\n",
       " 0.7948324680448554,\n",
       " 0.8207891077774719,\n",
       " 0.8136836427473949,\n",
       " 0.8968573317802891,\n",
       " 0.8885276561620238,\n",
       " 0.9712385446930345,\n",
       " 1.0040112158476142,\n",
       " 0.8263865903339425,\n",
       " 0.8428349138207232,\n",
       " 0.85007328199687,\n",
       " 0.7432331944637837,\n",
       " 0.8045445383167648,\n",
       " 0.8951373712309247,\n",
       " 0.7716757281290445,\n",
       " 0.8639293922651003,\n",
       " 0.8627765333894422,\n",
       " 0.8527520069988705,\n",
       " 0.7428328416055107,\n",
       " 0.7940021283526661,\n",
       " 0.8767724900655383,\n",
       " 0.9045713169047628,\n",
       " 0.812300395259087,\n",
       " 0.8056616122027982,\n",
       " 0.8001987597780855,\n",
       " 0.8004651110458783,\n",
       " 0.6472906919822714,\n",
       " 0.810430886653991,\n",
       " 0.8976207125406312,\n",
       " 0.9604665162964037,\n",
       " 0.8625371400047598,\n",
       " 0.6981140347320712,\n",
       " 0.9136189518567902,\n",
       " 0.7719734150114687,\n",
       " 0.791137798596622,\n",
       " 0.7425361215309668,\n",
       " 0.8001573361400157,\n",
       " 0.7538352762100466,\n",
       " 0.716149342191931,\n",
       " 0.8233606228938164,\n",
       " 0.8512200607418844,\n",
       " 0.8679703917076932,\n",
       " 0.7269399260553019,\n",
       " 0.7770101949377284,\n",
       " 0.9172429762728879,\n",
       " 0.7915274637828521,\n",
       " 0.8389094752123415,\n",
       " 0.8499032742057677,\n",
       " 0.771859299738785,\n",
       " 0.8565229218258855,\n",
       " 0.8852513544034416,\n",
       " 0.775662685308256,\n",
       " 0.677876387787432,\n",
       " 0.8130528882231935,\n",
       " 0.6430782257494049,\n",
       " 0.7429444349660136,\n",
       " 0.8040025458648654,\n",
       " 0.7081791423268519,\n",
       " 0.7179224312919524,\n",
       " 0.7265679223166586,\n",
       " 0.6722622529143455,\n",
       " 0.913544592730467,\n",
       " 0.7765768007301854,\n",
       " 0.7469280933551675,\n",
       " 0.723703726352578,\n",
       " 0.7797057884664805,\n",
       " 0.8082610267039904,\n",
       " 0.8777415215474729,\n",
       " 0.8566784434830068,\n",
       " 0.8147295427724687,\n",
       " 0.9238837662572448,\n",
       " 0.6794410404876031,\n",
       " 0.7503929650836705,\n",
       " 0.6596849356900886,\n",
       " 0.692521435772509,\n",
       " 0.6686611053482531,\n",
       " 0.8148365812160467,\n",
       " 0.7990831168506024,\n",
       " 0.7694981971961039,\n",
       " 0.6580932196462832,\n",
       " 0.7279656822277443,\n",
       " 0.8704382396749554,\n",
       " 0.7752560764880206,\n",
       " 0.7647169728074601,\n",
       " 0.6722207938681923,\n",
       " 0.7457677647307347,\n",
       " 0.6490891329532567,\n",
       " 0.7459902328526303,\n",
       " 0.7988763924749519,\n",
       " 0.5994663501968486,\n",
       " 0.7019337068231342,\n",
       " 0.6873141693802425,\n",
       " 0.6429266534473438,\n",
       " 0.7095524186110245,\n",
       " 0.7492155556842315,\n",
       " 0.7500402242632253,\n",
       " 0.7507046793509935,\n",
       " 0.7471025421154514,\n",
       " 0.6291624080883956,\n",
       " 0.7249872266435546,\n",
       " 0.6733618178225466,\n",
       " 0.88991469313432,\n",
       " 0.7244005545895863,\n",
       " 0.6698744217000454,\n",
       " 0.8354277149881818,\n",
       " 0.6354452788142038,\n",
       " 0.7307688869916094,\n",
       " 0.6500866371156064,\n",
       " 0.7352623712768905,\n",
       " 0.6664504898978045,\n",
       " 0.6543542743973451,\n",
       " 0.6928717686805791,\n",
       " 0.6531190351749828,\n",
       " 0.7581250092074916,\n",
       " 0.6034624264029049,\n",
       " 0.579881764590662,\n",
       " 0.7445718555449133,\n",
       " 0.766518003287282,\n",
       " 0.5916305992367705,\n",
       " 0.7482546659258638,\n",
       " 0.7327850553456338,\n",
       " 0.633374250564278,\n",
       " 0.7001516990424957,\n",
       " 0.5716237412016667,\n",
       " 0.6716480498926672,\n",
       " 0.7522366264134509,\n",
       " 0.6697881505097948,\n",
       " 0.7020900927040341,\n",
       " 0.6319978566005295,\n",
       " 0.7668491385976252,\n",
       " 0.7380641374702644,\n",
       " 0.7008577367038485,\n",
       " 0.7226167469989113,\n",
       " 0.8164117685999588,\n",
       " 0.5924208220719875,\n",
       " 0.6601271890634004,\n",
       " 0.6732461528438417,\n",
       " 0.6260511725603753,\n",
       " 0.7232613489522461,\n",
       " 0.7478748032853479,\n",
       " 0.6616231610930013,\n",
       " 0.7686584740974389,\n",
       " 0.6346885827885724,\n",
       " 0.6690700303053911,\n",
       " 0.602429869540696,\n",
       " 0.6814870931636346,\n",
       " 0.7183698506372742,\n",
       " 0.6878780361969096,\n",
       " 0.7378185801645304,\n",
       " 0.5958546888191821,\n",
       " 0.7349348325775005,\n",
       " 0.7573294444230902,\n",
       " 0.6539594368895437,\n",
       " 0.5916143970226295,\n",
       " 0.6594506980608383,\n",
       " 0.6866358244847355,\n",
       " 0.6558225782711514,\n",
       " 0.5921088255252522,\n",
       " 0.693652002070622,\n",
       " 0.6211371739512417,\n",
       " 0.6598559944757503,\n",
       " 0.676729812454966,\n",
       " 0.5734967181575206,\n",
       " 0.784656547690582,\n",
       " 0.6119398385136964,\n",
       " 0.6235941040928278,\n",
       " 0.6964833323815308,\n",
       " 0.5792206388245518,\n",
       " 0.813763416344928,\n",
       " 0.676611288726697,\n",
       " 0.5419566038546013,\n",
       " 0.6798197973090515,\n",
       " 0.6437064237227242,\n",
       " 0.611126524184072,\n",
       " 0.6730648929996051,\n",
       " 0.7557902567113988,\n",
       " 0.7017046161560568,\n",
       " 0.6886535164185958,\n",
       " 0.7101515411884618,\n",
       " 0.6445285471470374,\n",
       " 0.7051057592656522,\n",
       " 0.6820752881382433,\n",
       " 0.756370500640992,\n",
       " 0.6050540770571267,\n",
       " 0.7642131827467225,\n",
       " 0.660552510976498,\n",
       " 0.6161759520692562,\n",
       " 0.6509317172669864,\n",
       " 0.6141714081744432,\n",
       " 0.6109013411076374,\n",
       " 0.6671354576641836,\n",
       " 0.7151940974614263,\n",
       " 0.6101954636481781,\n",
       " 0.57143953635605,\n",
       " 0.650452314818236,\n",
       " 0.6684571560888056,\n",
       " 0.7105118300443608,\n",
       " 0.593949463662678,\n",
       " 0.7285978191960616,\n",
       " 0.5308381903685746,\n",
       " 0.7541967938703121,\n",
       " 0.6377736704537188,\n",
       " 0.6290860458169074,\n",
       " 0.6124721732775076,\n",
       " 0.5660954293496115,\n",
       " 0.659080537423257,\n",
       " 0.7235653401230525,\n",
       " 0.6677277065173867,\n",
       " 0.6160633828711227,\n",
       " 0.5296503569129988,\n",
       " 0.7575776829469847,\n",
       " 0.6678991572082512,\n",
       " 0.49692765648759357,\n",
       " 0.6430813839680398,\n",
       " 0.8280097273041477,\n",
       " 0.7854425135907903,\n",
       " 0.6581813345991789,\n",
       " 0.7098986653774745,\n",
       " 0.5724981172553654,\n",
       " 0.7122299458175854,\n",
       " 0.5800783658583539,\n",
       " 0.6374616912573474,\n",
       " 0.6222815938439429,\n",
       " 0.5608599225441429,\n",
       " 0.5696035502233218,\n",
       " 0.5462879858904494,\n",
       " 0.5921069526340762,\n",
       " 0.696257638960148,\n",
       " 0.6611852157704092,\n",
       " 0.6946571482517085,\n",
       " 0.6764540066178393,\n",
       " 0.6318243538969637,\n",
       " 0.6058370576767382,\n",
       " 0.46086455637890145,\n",
       " 0.6612081197254602,\n",
       " 0.577426154360303,\n",
       " 0.5340568070816212,\n",
       " 0.6983211739735193,\n",
       " 0.6692531461262686,\n",
       " 0.529514735612402,\n",
       " 0.698036035111329,\n",
       " 0.6813631390188902,\n",
       " 0.5794811785841314,\n",
       " 0.561941889053716,\n",
       " 0.5790798074155095,\n",
       " 0.6391998988986356,\n",
       " 0.669374177689139,\n",
       " 0.6488763520750499,\n",
       " 0.6516351150189083,\n",
       " 0.6305438989318283,\n",
       " 0.6064647520291776,\n",
       " 0.7473785350569077,\n",
       " 0.501462260630725,\n",
       " 0.7222213632008077,\n",
       " 0.5980442459265438,\n",
       " 0.573356320049644,\n",
       " 0.5307630976903418,\n",
       " 0.7658292858672607,\n",
       " 0.5866501878004574,\n",
       " 0.5992926381464353,\n",
       " 0.656055913395046,\n",
       " 0.6035485070559818,\n",
       " 0.6887191678645366,\n",
       " 0.5696559060925402,\n",
       " 0.6217426821897044,\n",
       " 0.5952705124531814,\n",
       " 0.6341925423606787,\n",
       " 0.6444934003443313,\n",
       " 0.7705647447955646,\n",
       " 0.6082453813850802,\n",
       " 0.6246969873449227,\n",
       " 0.5523105997748747,\n",
       " 0.47423787965510167,\n",
       " 0.5867339243862494,\n",
       " 0.49745632099173903,\n",
       " 0.5673640267623987,\n",
       " 0.5048953960069738,\n",
       " 0.578702834957256,\n",
       " 0.5437198720735668,\n",
       " 0.6130697532712066,\n",
       " 0.5398618403968468,\n",
       " 0.5986750955113374,\n",
       " 0.6191544518155652,\n",
       " 0.5681228031759575,\n",
       " 0.5297680301107274,\n",
       " 0.6317669334300268,\n",
       " 0.6320167754033884,\n",
       " 0.6129085673584248,\n",
       " 0.6099814604930129,\n",
       " 0.4903200042414412,\n",
       " 0.6455563070306338,\n",
       " 0.7472058160924412,\n",
       " 0.6395609221985007,\n",
       " 0.6464064152609602,\n",
       " 0.6602246144738301,\n",
       " 0.5490613351436525,\n",
       " 0.6306123998848652,\n",
       " 0.47612569260429566,\n",
       " 0.6303407027059684,\n",
       " 0.5430334756277033,\n",
       " 0.6887724430037574,\n",
       " 0.6096186441968023,\n",
       " 0.5332670393076749,\n",
       " 0.6730864438697565,\n",
       " 0.6338703071592036,\n",
       " 0.712174193106484,\n",
       " 0.5825276798772362,\n",
       " 0.621231834715577,\n",
       " 0.5242306421814159,\n",
       " 0.6175949514110117,\n",
       " 0.5532284116769187,\n",
       " 0.5333401462117844,\n",
       " 0.5755695990483379,\n",
       " 0.4288926381658043,\n",
       " 0.5052378766937777,\n",
       " 0.5734764390847475,\n",
       " 0.5532324912733722,\n",
       " 0.58411404136881,\n",
       " 0.6766498170162114,\n",
       " 0.5459934301087553,\n",
       " 0.5688983943406383,\n",
       " 0.4772041922228459,\n",
       " 0.5118712359255713,\n",
       " 0.5418274864003481,\n",
       " 0.49734157282917046,\n",
       " 0.5042676242105579,\n",
       " 0.6942415402977589,\n",
       " 0.5064077100248564,\n",
       " 0.5083765838002156,\n",
       " 0.6804292399141642,\n",
       " 0.6514599195065587,\n",
       " 0.47536464808923706,\n",
       " 0.5456331873373115,\n",
       " 0.5668360685731475,\n",
       " 0.5788603117290632,\n",
       " 0.5192998633091803,\n",
       " 0.44207201786564676,\n",
       " 0.5447967762469529,\n",
       " 0.47235468645035383,\n",
       " 0.6027797626284569,\n",
       " 0.5538654404471041,\n",
       " 0.6030073688635698,\n",
       " 0.5068588750654035,\n",
       " 0.6022295579048133,\n",
       " 0.48521464384814494,\n",
       " 0.5878459807742495,\n",
       " 0.5099063664744885,\n",
       " 0.5754167737847594,\n",
       " 0.5110593161471706,\n",
       " 0.5253869209044244,\n",
       " 0.5854240983852206,\n",
       " 0.4419896300696695,\n",
       " 0.5913264437440687,\n",
       " 0.5841623153197033,\n",
       " 0.5440503342122913,\n",
       " 0.4997232865190041,\n",
       " 0.6080876038702611,\n",
       " 0.5122186880582484,\n",
       " 0.5901984039040306,\n",
       " 0.5292814987442795,\n",
       " 0.4894284308079531,\n",
       " 0.43413763878304,\n",
       " 0.5257607121830381,\n",
       " 0.5683124782298953,\n",
       " 0.615514884601764,\n",
       " 0.4949560106223252,\n",
       " 0.5314987938428931,\n",
       " 0.5300311972896714,\n",
       " 0.6967374546036725,\n",
       " 0.6211281263091637,\n",
       " 0.5836928524061681,\n",
       " 0.5842321610671735,\n",
       " 0.6574716639028995,\n",
       " 0.5421573290869,\n",
       " 0.5298028865863618,\n",
       " 0.5584600724099515,\n",
       " 0.5560802196211376,\n",
       " 0.6498643331588745,\n",
       " 0.5898653255775218,\n",
       " 0.5013936343195947,\n",
       " 0.6311403326758136,\n",
       " 0.47776636139158846,\n",
       " 0.42749212965682665,\n",
       " 0.6782170817415382,\n",
       " 0.5584066366819763,\n",
       " 0.5214269172281145,\n",
       " 0.5334596073356592,\n",
       " 0.426411922243281,\n",
       " 0.5326029459377171,\n",
       " 0.5023783107471333,\n",
       " 0.523260506795948,\n",
       " 0.5253082011840586,\n",
       " 0.5608211337892426,\n",
       " 0.527918057638651,\n",
       " 0.5396477594639473,\n",
       " ...]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.11236666666666667, 0.1135\n",
      "train acc, test acc : 0.7894666666666666, 0.7906\n",
      "train acc, test acc : 0.8781333333333333, 0.8829\n",
      "train acc, test acc : 0.8992166666666667, 0.9034\n",
      "train acc, test acc : 0.9073166666666667, 0.9091\n",
      "train acc, test acc : 0.913, 0.916\n",
      "train acc, test acc : 0.9184833333333333, 0.9203\n",
      "train acc, test acc : 0.9218333333333333, 0.9237\n",
      "train acc, test acc : 0.9260333333333334, 0.9276\n",
      "train acc, test acc : 0.9300833333333334, 0.9305\n",
      "train acc, test acc : 0.9320166666666667, 0.9328\n",
      "train acc, test acc : 0.9343666666666667, 0.9337\n",
      "train acc, test acc : 0.9359, 0.9359\n",
      "train acc, test acc : 0.9389666666666666, 0.9364\n",
      "train acc, test acc : 0.94145, 0.9396\n",
      "train acc, test acc : 0.9425166666666667, 0.9407\n",
      "train acc, test acc : 0.9441666666666667, 0.9427\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # mini batch size\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# iterate number per one epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
