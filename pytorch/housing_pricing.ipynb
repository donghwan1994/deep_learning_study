{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('./input/train.csv')\n",
    "X_test = pd.read_csv('./input/test.csv')\n",
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = pd.get_dummies(data, dummy_na=True, drop_first=True)\n",
    "data.drop('Id', axis=1, inplace=True)\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data['SalePrice']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.235294     0.150685  0.033420     0.666667        0.500   0.949275   \n",
       "1    0.000000     0.202055  0.038795     0.555556        0.875   0.753623   \n",
       "2    0.235294     0.160959  0.046507     0.666667        0.500   0.934783   \n",
       "3    0.294118     0.133562  0.038561     0.666667        0.500   0.311594   \n",
       "4    0.235294     0.215753  0.060576     0.777778        0.500   0.927536   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0      0.883333     0.12250    0.125089         0.0  ...           0.0   \n",
       "1      0.433333     0.00000    0.173281         0.0  ...           0.0   \n",
       "2      0.866667     0.10125    0.086109         0.0  ...           0.0   \n",
       "3      0.333333     0.00000    0.038271         0.0  ...           0.0   \n",
       "4      0.833333     0.21875    0.116052         0.0  ...           0.0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0           0.0          1.0           0.0                    0.0   \n",
       "1           0.0          1.0           0.0                    0.0   \n",
       "2           0.0          1.0           0.0                    0.0   \n",
       "3           0.0          1.0           0.0                    0.0   \n",
       "4           0.0          1.0           0.0                    0.0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                   0.0                   0.0                   1.0   \n",
       "1                   0.0                   0.0                   1.0   \n",
       "2                   0.0                   0.0                   1.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   1.0   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                    0.0                0.0  \n",
       "1                    0.0                0.0  \n",
       "2                    0.0                0.0  \n",
       "3                    0.0                0.0  \n",
       "4                    0.0                0.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train = data.iloc[:1460]\n",
    "test = data.iloc[1460:]\n",
    "test.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 288)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(288, 144)\n",
    "        self.fc2 = nn.Linear(144, 72)\n",
    "        self.fc3 = nn.Linear(72, 18)\n",
    "        self.fc4 = nn.Linear(18, 1)\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        #x = self.dropout(F.relu(self.fc2(x)))\n",
    "        #x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Regressor()\n",
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300..  Training Loss: 10.595..  Test Loss: 8.432.. \n",
      "Epoch: 2/300..  Training Loss: 7.461..  Test Loss: 6.610.. \n",
      "Epoch: 3/300..  Training Loss: 6.083..  Test Loss: 5.550.. \n",
      "Epoch: 4/300..  Training Loss: 5.186..  Test Loss: 4.781.. \n",
      "Epoch: 5/300..  Training Loss: 4.495..  Test Loss: 4.164.. \n",
      "Epoch: 6/300..  Training Loss: 3.933..  Test Loss: 3.653.. \n",
      "Epoch: 7/300..  Training Loss: 3.461..  Test Loss: 3.222.. \n",
      "Epoch: 8/300..  Training Loss: 3.061..  Test Loss: 2.852.. \n",
      "Epoch: 9/300..  Training Loss: 2.715..  Test Loss: 2.529.. \n",
      "Epoch: 10/300..  Training Loss: 2.408..  Test Loss: 2.241.. \n",
      "Epoch: 11/300..  Training Loss: 2.133..  Test Loss: 1.980.. \n",
      "Epoch: 12/300..  Training Loss: 1.883..  Test Loss: 1.741.. \n",
      "Epoch: 13/300..  Training Loss: 1.652..  Test Loss: 1.520.. \n",
      "Epoch: 14/300..  Training Loss: 1.435..  Test Loss: 1.313.. \n",
      "Epoch: 15/300..  Training Loss: 1.232..  Test Loss: 1.119.. \n",
      "Epoch: 16/300..  Training Loss: 1.043..  Test Loss: 0.941.. \n",
      "Epoch: 17/300..  Training Loss: 0.869..  Test Loss: 0.781.. \n",
      "Epoch: 18/300..  Training Loss: 0.714..  Test Loss: 0.642.. \n",
      "Epoch: 19/300..  Training Loss: 0.580..  Test Loss: 0.531.. \n",
      "Epoch: 20/300..  Training Loss: 0.475..  Test Loss: 0.453.. \n",
      "Epoch: 21/300..  Training Loss: 0.404..  Test Loss: 0.408.. \n",
      "Epoch: 22/300..  Training Loss: 0.364..  Test Loss: 0.387.. \n",
      "Epoch: 23/300..  Training Loss: 0.345..  Test Loss: 0.378.. \n",
      "Epoch: 24/300..  Training Loss: 0.335..  Test Loss: 0.372.. \n",
      "Epoch: 25/300..  Training Loss: 0.329..  Test Loss: 0.368.. \n",
      "Epoch: 26/300..  Training Loss: 0.324..  Test Loss: 0.364.. \n",
      "Epoch: 27/300..  Training Loss: 0.320..  Test Loss: 0.360.. \n",
      "Epoch: 28/300..  Training Loss: 0.315..  Test Loss: 0.355.. \n",
      "Epoch: 29/300..  Training Loss: 0.311..  Test Loss: 0.350.. \n",
      "Epoch: 30/300..  Training Loss: 0.306..  Test Loss: 0.346.. \n",
      "Epoch: 31/300..  Training Loss: 0.301..  Test Loss: 0.341.. \n",
      "Epoch: 32/300..  Training Loss: 0.297..  Test Loss: 0.336.. \n",
      "Epoch: 33/300..  Training Loss: 0.292..  Test Loss: 0.331.. \n",
      "Epoch: 34/300..  Training Loss: 0.287..  Test Loss: 0.326.. \n",
      "Epoch: 35/300..  Training Loss: 0.282..  Test Loss: 0.320.. \n",
      "Epoch: 36/300..  Training Loss: 0.278..  Test Loss: 0.315.. \n",
      "Epoch: 37/300..  Training Loss: 0.273..  Test Loss: 0.310.. \n",
      "Epoch: 38/300..  Training Loss: 0.268..  Test Loss: 0.305.. \n",
      "Epoch: 39/300..  Training Loss: 0.263..  Test Loss: 0.300.. \n",
      "Epoch: 40/300..  Training Loss: 0.258..  Test Loss: 0.294.. \n",
      "Epoch: 41/300..  Training Loss: 0.254..  Test Loss: 0.289.. \n",
      "Epoch: 42/300..  Training Loss: 0.249..  Test Loss: 0.285.. \n",
      "Epoch: 43/300..  Training Loss: 0.245..  Test Loss: 0.280.. \n",
      "Epoch: 44/300..  Training Loss: 0.241..  Test Loss: 0.275.. \n",
      "Epoch: 45/300..  Training Loss: 0.237..  Test Loss: 0.271.. \n",
      "Epoch: 46/300..  Training Loss: 0.233..  Test Loss: 0.266.. \n",
      "Epoch: 47/300..  Training Loss: 0.229..  Test Loss: 0.262.. \n",
      "Epoch: 48/300..  Training Loss: 0.226..  Test Loss: 0.259.. \n",
      "Epoch: 49/300..  Training Loss: 0.223..  Test Loss: 0.255.. \n",
      "Epoch: 50/300..  Training Loss: 0.219..  Test Loss: 0.252.. \n",
      "Epoch: 51/300..  Training Loss: 0.217..  Test Loss: 0.248.. \n",
      "Epoch: 52/300..  Training Loss: 0.214..  Test Loss: 0.245.. \n",
      "Epoch: 53/300..  Training Loss: 0.211..  Test Loss: 0.243.. \n",
      "Epoch: 54/300..  Training Loss: 0.209..  Test Loss: 0.240.. \n",
      "Epoch: 55/300..  Training Loss: 0.207..  Test Loss: 0.238.. \n",
      "Epoch: 56/300..  Training Loss: 0.205..  Test Loss: 0.235.. \n",
      "Epoch: 57/300..  Training Loss: 0.203..  Test Loss: 0.233.. \n",
      "Epoch: 58/300..  Training Loss: 0.201..  Test Loss: 0.231.. \n",
      "Epoch: 59/300..  Training Loss: 0.199..  Test Loss: 0.229.. \n",
      "Epoch: 60/300..  Training Loss: 0.197..  Test Loss: 0.227.. \n",
      "Epoch: 61/300..  Training Loss: 0.196..  Test Loss: 0.226.. \n",
      "Epoch: 62/300..  Training Loss: 0.194..  Test Loss: 0.224.. \n",
      "Epoch: 63/300..  Training Loss: 0.192..  Test Loss: 0.222.. \n",
      "Epoch: 64/300..  Training Loss: 0.191..  Test Loss: 0.221.. \n",
      "Epoch: 65/300..  Training Loss: 0.189..  Test Loss: 0.219.. \n",
      "Epoch: 66/300..  Training Loss: 0.188..  Test Loss: 0.218.. \n",
      "Epoch: 67/300..  Training Loss: 0.186..  Test Loss: 0.217.. \n",
      "Epoch: 68/300..  Training Loss: 0.185..  Test Loss: 0.215.. \n",
      "Epoch: 69/300..  Training Loss: 0.183..  Test Loss: 0.214.. \n",
      "Epoch: 70/300..  Training Loss: 0.182..  Test Loss: 0.213.. \n",
      "Epoch: 71/300..  Training Loss: 0.180..  Test Loss: 0.211.. \n",
      "Epoch: 72/300..  Training Loss: 0.179..  Test Loss: 0.210.. \n",
      "Epoch: 73/300..  Training Loss: 0.177..  Test Loss: 0.209.. \n",
      "Epoch: 74/300..  Training Loss: 0.176..  Test Loss: 0.208.. \n",
      "Epoch: 75/300..  Training Loss: 0.174..  Test Loss: 0.206.. \n",
      "Epoch: 76/300..  Training Loss: 0.173..  Test Loss: 0.205.. \n",
      "Epoch: 77/300..  Training Loss: 0.172..  Test Loss: 0.204.. \n",
      "Epoch: 78/300..  Training Loss: 0.170..  Test Loss: 0.203.. \n",
      "Epoch: 79/300..  Training Loss: 0.169..  Test Loss: 0.202.. \n",
      "Epoch: 80/300..  Training Loss: 0.168..  Test Loss: 0.201.. \n",
      "Epoch: 81/300..  Training Loss: 0.166..  Test Loss: 0.200.. \n",
      "Epoch: 82/300..  Training Loss: 0.165..  Test Loss: 0.199.. \n",
      "Epoch: 83/300..  Training Loss: 0.164..  Test Loss: 0.198.. \n",
      "Epoch: 84/300..  Training Loss: 0.162..  Test Loss: 0.197.. \n",
      "Epoch: 85/300..  Training Loss: 0.161..  Test Loss: 0.196.. \n",
      "Epoch: 86/300..  Training Loss: 0.160..  Test Loss: 0.195.. \n",
      "Epoch: 87/300..  Training Loss: 0.159..  Test Loss: 0.194.. \n",
      "Epoch: 88/300..  Training Loss: 0.158..  Test Loss: 0.193.. \n",
      "Epoch: 89/300..  Training Loss: 0.156..  Test Loss: 0.192.. \n",
      "Epoch: 90/300..  Training Loss: 0.155..  Test Loss: 0.192.. \n",
      "Epoch: 91/300..  Training Loss: 0.154..  Test Loss: 0.191.. \n",
      "Epoch: 92/300..  Training Loss: 0.153..  Test Loss: 0.190.. \n",
      "Epoch: 93/300..  Training Loss: 0.152..  Test Loss: 0.189.. \n",
      "Epoch: 94/300..  Training Loss: 0.151..  Test Loss: 0.189.. \n",
      "Epoch: 95/300..  Training Loss: 0.150..  Test Loss: 0.188.. \n",
      "Epoch: 96/300..  Training Loss: 0.149..  Test Loss: 0.187.. \n",
      "Epoch: 97/300..  Training Loss: 0.148..  Test Loss: 0.187.. \n",
      "Epoch: 98/300..  Training Loss: 0.147..  Test Loss: 0.186.. \n",
      "Epoch: 99/300..  Training Loss: 0.146..  Test Loss: 0.185.. \n",
      "Epoch: 100/300..  Training Loss: 0.145..  Test Loss: 0.185.. \n",
      "Epoch: 101/300..  Training Loss: 0.144..  Test Loss: 0.184.. \n",
      "Epoch: 102/300..  Training Loss: 0.143..  Test Loss: 0.184.. \n",
      "Epoch: 103/300..  Training Loss: 0.143..  Test Loss: 0.183.. \n",
      "Epoch: 104/300..  Training Loss: 0.142..  Test Loss: 0.182.. \n",
      "Epoch: 105/300..  Training Loss: 0.141..  Test Loss: 0.182.. \n",
      "Epoch: 106/300..  Training Loss: 0.140..  Test Loss: 0.181.. \n",
      "Epoch: 107/300..  Training Loss: 0.139..  Test Loss: 0.181.. \n",
      "Epoch: 108/300..  Training Loss: 0.138..  Test Loss: 0.180.. \n",
      "Epoch: 109/300..  Training Loss: 0.137..  Test Loss: 0.180.. \n",
      "Epoch: 110/300..  Training Loss: 0.137..  Test Loss: 0.179.. \n",
      "Epoch: 111/300..  Training Loss: 0.136..  Test Loss: 0.179.. \n",
      "Epoch: 112/300..  Training Loss: 0.135..  Test Loss: 0.178.. \n",
      "Epoch: 113/300..  Training Loss: 0.134..  Test Loss: 0.178.. \n",
      "Epoch: 114/300..  Training Loss: 0.134..  Test Loss: 0.178.. \n",
      "Epoch: 115/300..  Training Loss: 0.133..  Test Loss: 0.177.. \n",
      "Epoch: 116/300..  Training Loss: 0.132..  Test Loss: 0.177.. \n",
      "Epoch: 117/300..  Training Loss: 0.131..  Test Loss: 0.176.. \n",
      "Epoch: 118/300..  Training Loss: 0.131..  Test Loss: 0.176.. \n",
      "Epoch: 119/300..  Training Loss: 0.130..  Test Loss: 0.175.. \n",
      "Epoch: 120/300..  Training Loss: 0.129..  Test Loss: 0.175.. \n",
      "Epoch: 121/300..  Training Loss: 0.128..  Test Loss: 0.174.. \n",
      "Epoch: 122/300..  Training Loss: 0.128..  Test Loss: 0.174.. \n",
      "Epoch: 123/300..  Training Loss: 0.127..  Test Loss: 0.173.. \n",
      "Epoch: 124/300..  Training Loss: 0.126..  Test Loss: 0.173.. \n",
      "Epoch: 125/300..  Training Loss: 0.126..  Test Loss: 0.173.. \n",
      "Epoch: 126/300..  Training Loss: 0.125..  Test Loss: 0.172.. \n",
      "Epoch: 127/300..  Training Loss: 0.124..  Test Loss: 0.172.. \n",
      "Epoch: 128/300..  Training Loss: 0.124..  Test Loss: 0.171.. \n",
      "Epoch: 129/300..  Training Loss: 0.123..  Test Loss: 0.171.. \n",
      "Epoch: 130/300..  Training Loss: 0.122..  Test Loss: 0.170.. \n",
      "Epoch: 131/300..  Training Loss: 0.122..  Test Loss: 0.170.. \n",
      "Epoch: 132/300..  Training Loss: 0.121..  Test Loss: 0.170.. \n",
      "Epoch: 133/300..  Training Loss: 0.121..  Test Loss: 0.169.. \n",
      "Epoch: 134/300..  Training Loss: 0.120..  Test Loss: 0.169.. \n",
      "Epoch: 135/300..  Training Loss: 0.119..  Test Loss: 0.168.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/300..  Training Loss: 0.119..  Test Loss: 0.168.. \n",
      "Epoch: 137/300..  Training Loss: 0.118..  Test Loss: 0.168.. \n",
      "Epoch: 138/300..  Training Loss: 0.118..  Test Loss: 0.167.. \n",
      "Epoch: 139/300..  Training Loss: 0.117..  Test Loss: 0.167.. \n",
      "Epoch: 140/300..  Training Loss: 0.117..  Test Loss: 0.166.. \n",
      "Epoch: 141/300..  Training Loss: 0.116..  Test Loss: 0.166.. \n",
      "Epoch: 142/300..  Training Loss: 0.116..  Test Loss: 0.166.. \n",
      "Epoch: 143/300..  Training Loss: 0.115..  Test Loss: 0.165.. \n",
      "Epoch: 144/300..  Training Loss: 0.115..  Test Loss: 0.165.. \n",
      "Epoch: 145/300..  Training Loss: 0.114..  Test Loss: 0.165.. \n",
      "Epoch: 146/300..  Training Loss: 0.114..  Test Loss: 0.164.. \n",
      "Epoch: 147/300..  Training Loss: 0.113..  Test Loss: 0.164.. \n",
      "Epoch: 148/300..  Training Loss: 0.113..  Test Loss: 0.163.. \n",
      "Epoch: 149/300..  Training Loss: 0.112..  Test Loss: 0.163.. \n",
      "Epoch: 150/300..  Training Loss: 0.112..  Test Loss: 0.163.. \n",
      "Epoch: 151/300..  Training Loss: 0.111..  Test Loss: 0.162.. \n",
      "Epoch: 152/300..  Training Loss: 0.111..  Test Loss: 0.162.. \n",
      "Epoch: 153/300..  Training Loss: 0.110..  Test Loss: 0.162.. \n",
      "Epoch: 154/300..  Training Loss: 0.110..  Test Loss: 0.161.. \n",
      "Epoch: 155/300..  Training Loss: 0.110..  Test Loss: 0.161.. \n",
      "Epoch: 156/300..  Training Loss: 0.109..  Test Loss: 0.161.. \n",
      "Epoch: 157/300..  Training Loss: 0.109..  Test Loss: 0.161.. \n",
      "Epoch: 158/300..  Training Loss: 0.108..  Test Loss: 0.160.. \n",
      "Epoch: 159/300..  Training Loss: 0.108..  Test Loss: 0.160.. \n",
      "Epoch: 160/300..  Training Loss: 0.107..  Test Loss: 0.160.. \n",
      "Epoch: 161/300..  Training Loss: 0.107..  Test Loss: 0.159.. \n",
      "Epoch: 162/300..  Training Loss: 0.107..  Test Loss: 0.159.. \n",
      "Epoch: 163/300..  Training Loss: 0.106..  Test Loss: 0.159.. \n",
      "Epoch: 164/300..  Training Loss: 0.106..  Test Loss: 0.158.. \n",
      "Epoch: 165/300..  Training Loss: 0.106..  Test Loss: 0.158.. \n",
      "Epoch: 166/300..  Training Loss: 0.105..  Test Loss: 0.158.. \n",
      "Epoch: 167/300..  Training Loss: 0.105..  Test Loss: 0.158.. \n",
      "Epoch: 168/300..  Training Loss: 0.104..  Test Loss: 0.158.. \n",
      "Epoch: 169/300..  Training Loss: 0.104..  Test Loss: 0.157.. \n",
      "Epoch: 170/300..  Training Loss: 0.104..  Test Loss: 0.157.. \n",
      "Epoch: 171/300..  Training Loss: 0.103..  Test Loss: 0.157.. \n",
      "Epoch: 172/300..  Training Loss: 0.103..  Test Loss: 0.157.. \n",
      "Epoch: 173/300..  Training Loss: 0.103..  Test Loss: 0.156.. \n",
      "Epoch: 174/300..  Training Loss: 0.102..  Test Loss: 0.156.. \n",
      "Epoch: 175/300..  Training Loss: 0.102..  Test Loss: 0.156.. \n",
      "Epoch: 176/300..  Training Loss: 0.102..  Test Loss: 0.156.. \n",
      "Epoch: 177/300..  Training Loss: 0.101..  Test Loss: 0.156.. \n",
      "Epoch: 178/300..  Training Loss: 0.101..  Test Loss: 0.155.. \n",
      "Epoch: 179/300..  Training Loss: 0.101..  Test Loss: 0.155.. \n",
      "Epoch: 180/300..  Training Loss: 0.100..  Test Loss: 0.155.. \n",
      "Epoch: 181/300..  Training Loss: 0.100..  Test Loss: 0.155.. \n",
      "Epoch: 182/300..  Training Loss: 0.100..  Test Loss: 0.155.. \n",
      "Epoch: 183/300..  Training Loss: 0.099..  Test Loss: 0.154.. \n",
      "Epoch: 184/300..  Training Loss: 0.099..  Test Loss: 0.154.. \n",
      "Epoch: 185/300..  Training Loss: 0.099..  Test Loss: 0.154.. \n",
      "Epoch: 186/300..  Training Loss: 0.098..  Test Loss: 0.154.. \n",
      "Epoch: 187/300..  Training Loss: 0.098..  Test Loss: 0.154.. \n",
      "Epoch: 188/300..  Training Loss: 0.098..  Test Loss: 0.153.. \n",
      "Epoch: 189/300..  Training Loss: 0.098..  Test Loss: 0.153.. \n",
      "Epoch: 190/300..  Training Loss: 0.097..  Test Loss: 0.153.. \n",
      "Epoch: 191/300..  Training Loss: 0.097..  Test Loss: 0.153.. \n",
      "Epoch: 192/300..  Training Loss: 0.097..  Test Loss: 0.153.. \n",
      "Epoch: 193/300..  Training Loss: 0.096..  Test Loss: 0.153.. \n",
      "Epoch: 194/300..  Training Loss: 0.096..  Test Loss: 0.153.. \n",
      "Epoch: 195/300..  Training Loss: 0.096..  Test Loss: 0.152.. \n",
      "Epoch: 196/300..  Training Loss: 0.096..  Test Loss: 0.152.. \n",
      "Epoch: 197/300..  Training Loss: 0.095..  Test Loss: 0.152.. \n",
      "Epoch: 198/300..  Training Loss: 0.095..  Test Loss: 0.152.. \n",
      "Epoch: 199/300..  Training Loss: 0.095..  Test Loss: 0.152.. \n",
      "Epoch: 200/300..  Training Loss: 0.094..  Test Loss: 0.152.. \n",
      "Epoch: 201/300..  Training Loss: 0.094..  Test Loss: 0.152.. \n",
      "Epoch: 202/300..  Training Loss: 0.094..  Test Loss: 0.151.. \n",
      "Epoch: 203/300..  Training Loss: 0.094..  Test Loss: 0.151.. \n",
      "Epoch: 204/300..  Training Loss: 0.093..  Test Loss: 0.151.. \n",
      "Epoch: 205/300..  Training Loss: 0.093..  Test Loss: 0.151.. \n",
      "Epoch: 206/300..  Training Loss: 0.093..  Test Loss: 0.151.. \n",
      "Epoch: 207/300..  Training Loss: 0.093..  Test Loss: 0.151.. \n",
      "Epoch: 208/300..  Training Loss: 0.092..  Test Loss: 0.151.. \n",
      "Epoch: 209/300..  Training Loss: 0.092..  Test Loss: 0.151.. \n",
      "Epoch: 210/300..  Training Loss: 0.092..  Test Loss: 0.150.. \n",
      "Epoch: 211/300..  Training Loss: 0.092..  Test Loss: 0.150.. \n",
      "Epoch: 212/300..  Training Loss: 0.091..  Test Loss: 0.150.. \n",
      "Epoch: 213/300..  Training Loss: 0.091..  Test Loss: 0.150.. \n",
      "Epoch: 214/300..  Training Loss: 0.091..  Test Loss: 0.150.. \n",
      "Epoch: 215/300..  Training Loss: 0.091..  Test Loss: 0.150.. \n",
      "Epoch: 216/300..  Training Loss: 0.090..  Test Loss: 0.150.. \n",
      "Epoch: 217/300..  Training Loss: 0.090..  Test Loss: 0.150.. \n",
      "Epoch: 218/300..  Training Loss: 0.090..  Test Loss: 0.150.. \n",
      "Epoch: 219/300..  Training Loss: 0.090..  Test Loss: 0.150.. \n",
      "Epoch: 220/300..  Training Loss: 0.090..  Test Loss: 0.150.. \n",
      "Epoch: 221/300..  Training Loss: 0.089..  Test Loss: 0.149.. \n",
      "Epoch: 222/300..  Training Loss: 0.089..  Test Loss: 0.149.. \n",
      "Epoch: 223/300..  Training Loss: 0.089..  Test Loss: 0.149.. \n",
      "Epoch: 224/300..  Training Loss: 0.089..  Test Loss: 0.149.. \n",
      "Epoch: 225/300..  Training Loss: 0.088..  Test Loss: 0.149.. \n",
      "Epoch: 226/300..  Training Loss: 0.088..  Test Loss: 0.149.. \n",
      "Epoch: 227/300..  Training Loss: 0.088..  Test Loss: 0.149.. \n",
      "Epoch: 228/300..  Training Loss: 0.088..  Test Loss: 0.149.. \n",
      "Epoch: 229/300..  Training Loss: 0.088..  Test Loss: 0.149.. \n",
      "Epoch: 230/300..  Training Loss: 0.087..  Test Loss: 0.149.. \n",
      "Epoch: 231/300..  Training Loss: 0.087..  Test Loss: 0.149.. \n",
      "Epoch: 232/300..  Training Loss: 0.087..  Test Loss: 0.149.. \n",
      "Epoch: 233/300..  Training Loss: 0.087..  Test Loss: 0.149.. \n",
      "Epoch: 234/300..  Training Loss: 0.087..  Test Loss: 0.149.. \n",
      "Epoch: 235/300..  Training Loss: 0.086..  Test Loss: 0.149.. \n",
      "Epoch: 236/300..  Training Loss: 0.086..  Test Loss: 0.148.. \n",
      "Epoch: 237/300..  Training Loss: 0.086..  Test Loss: 0.148.. \n",
      "Epoch: 238/300..  Training Loss: 0.086..  Test Loss: 0.148.. \n",
      "Epoch: 239/300..  Training Loss: 0.086..  Test Loss: 0.148.. \n",
      "Epoch: 240/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 241/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 242/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 243/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 244/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 245/300..  Training Loss: 0.085..  Test Loss: 0.148.. \n",
      "Epoch: 246/300..  Training Loss: 0.084..  Test Loss: 0.148.. \n",
      "Epoch: 247/300..  Training Loss: 0.084..  Test Loss: 0.148.. \n",
      "Epoch: 248/300..  Training Loss: 0.084..  Test Loss: 0.148.. \n",
      "Epoch: 249/300..  Training Loss: 0.084..  Test Loss: 0.148.. \n",
      "Epoch: 250/300..  Training Loss: 0.084..  Test Loss: 0.148.. \n",
      "Epoch: 251/300..  Training Loss: 0.083..  Test Loss: 0.148.. \n",
      "Epoch: 252/300..  Training Loss: 0.083..  Test Loss: 0.148.. \n",
      "Epoch: 253/300..  Training Loss: 0.083..  Test Loss: 0.148.. \n",
      "Epoch: 254/300..  Training Loss: 0.083..  Test Loss: 0.148.. \n",
      "Epoch: 255/300..  Training Loss: 0.083..  Test Loss: 0.148.. \n",
      "Epoch: 256/300..  Training Loss: 0.082..  Test Loss: 0.148.. \n",
      "Epoch: 257/300..  Training Loss: 0.082..  Test Loss: 0.148.. \n",
      "Epoch: 258/300..  Training Loss: 0.082..  Test Loss: 0.147.. \n",
      "Epoch: 259/300..  Training Loss: 0.082..  Test Loss: 0.148.. \n",
      "Epoch: 260/300..  Training Loss: 0.082..  Test Loss: 0.147.. \n",
      "Epoch: 261/300..  Training Loss: 0.081..  Test Loss: 0.147.. \n",
      "Epoch: 262/300..  Training Loss: 0.081..  Test Loss: 0.147.. \n",
      "Epoch: 263/300..  Training Loss: 0.081..  Test Loss: 0.147.. \n",
      "Epoch: 264/300..  Training Loss: 0.081..  Test Loss: 0.147.. \n",
      "Epoch: 265/300..  Training Loss: 0.081..  Test Loss: 0.147.. \n",
      "Epoch: 266/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n",
      "Epoch: 267/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n",
      "Epoch: 268/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n",
      "Epoch: 270/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n",
      "Epoch: 271/300..  Training Loss: 0.080..  Test Loss: 0.147.. \n",
      "Epoch: 272/300..  Training Loss: 0.079..  Test Loss: 0.147.. \n",
      "Epoch: 273/300..  Training Loss: 0.079..  Test Loss: 0.147.. \n",
      "Epoch: 274/300..  Training Loss: 0.079..  Test Loss: 0.147.. \n",
      "Epoch: 275/300..  Training Loss: 0.079..  Test Loss: 0.147.. \n",
      "Epoch: 276/300..  Training Loss: 0.079..  Test Loss: 0.147.. \n",
      "Epoch: 277/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 278/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 279/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 280/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 281/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 282/300..  Training Loss: 0.078..  Test Loss: 0.147.. \n",
      "Epoch: 283/300..  Training Loss: 0.077..  Test Loss: 0.147.. \n",
      "Epoch: 284/300..  Training Loss: 0.077..  Test Loss: 0.147.. \n",
      "Epoch: 285/300..  Training Loss: 0.077..  Test Loss: 0.147.. \n",
      "Epoch: 286/300..  Training Loss: 0.077..  Test Loss: 0.147.. \n",
      "Epoch: 287/300..  Training Loss: 0.077..  Test Loss: 0.147.. \n",
      "Epoch: 288/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 289/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 290/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 291/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 292/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 293/300..  Training Loss: 0.076..  Test Loss: 0.147.. \n",
      "Epoch: 294/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 295/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 296/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 297/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 298/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 299/300..  Training Loss: 0.075..  Test Loss: 0.147.. \n",
      "Epoch: 300/300..  Training Loss: 0.074..  Test Loss: 0.147.. \n"
     ]
    }
   ],
   "source": [
    "model = Regressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20b74f564e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./input/sample_submission.csv')\n",
    "submission['SalePrice'] = output.numpy()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>128364.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>157989.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>182233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>186996.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>200391.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1466</td>\n",
       "      <td>171916.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467</td>\n",
       "      <td>174920.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>154836.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1469</td>\n",
       "      <td>180462.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1470</td>\n",
       "      <td>130878.476562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1471</td>\n",
       "      <td>161968.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1472</td>\n",
       "      <td>103332.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1473</td>\n",
       "      <td>102970.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1474</td>\n",
       "      <td>139547.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1475</td>\n",
       "      <td>102398.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1476</td>\n",
       "      <td>362216.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1477</td>\n",
       "      <td>244518.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1478</td>\n",
       "      <td>300017.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1479</td>\n",
       "      <td>307442.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1480</td>\n",
       "      <td>467588.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1481</td>\n",
       "      <td>314933.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1482</td>\n",
       "      <td>200260.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1483</td>\n",
       "      <td>174575.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1484</td>\n",
       "      <td>163160.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1485</td>\n",
       "      <td>177237.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1486</td>\n",
       "      <td>184873.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1487</td>\n",
       "      <td>342014.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1488</td>\n",
       "      <td>225092.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1489</td>\n",
       "      <td>197313.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1490</td>\n",
       "      <td>240105.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2890</td>\n",
       "      <td>90471.804688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2891</td>\n",
       "      <td>144831.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2892</td>\n",
       "      <td>37231.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2893</td>\n",
       "      <td>73736.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2894</td>\n",
       "      <td>43988.050781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2895</td>\n",
       "      <td>352525.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2896</td>\n",
       "      <td>286334.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2897</td>\n",
       "      <td>196819.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2898</td>\n",
       "      <td>142756.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2899</td>\n",
       "      <td>204118.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2900</td>\n",
       "      <td>165214.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2901</td>\n",
       "      <td>242796.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2902</td>\n",
       "      <td>170066.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2903</td>\n",
       "      <td>317440.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2904</td>\n",
       "      <td>349227.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2905</td>\n",
       "      <td>56490.425781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2906</td>\n",
       "      <td>174882.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2907</td>\n",
       "      <td>100538.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2908</td>\n",
       "      <td>134884.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2909</td>\n",
       "      <td>140087.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2910</td>\n",
       "      <td>75793.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2911</td>\n",
       "      <td>79643.992188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2912</td>\n",
       "      <td>149441.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2913</td>\n",
       "      <td>70645.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2914</td>\n",
       "      <td>73459.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>82787.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>73339.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>165991.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>117034.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>206727.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  128364.875000\n",
       "1     1462  157989.125000\n",
       "2     1463  182233.000000\n",
       "3     1464  186996.046875\n",
       "4     1465  200391.906250\n",
       "5     1466  171916.250000\n",
       "6     1467  174920.703125\n",
       "7     1468  154836.984375\n",
       "8     1469  180462.125000\n",
       "9     1470  130878.476562\n",
       "10    1471  161968.375000\n",
       "11    1472  103332.015625\n",
       "12    1473  102970.312500\n",
       "13    1474  139547.562500\n",
       "14    1475  102398.656250\n",
       "15    1476  362216.843750\n",
       "16    1477  244518.390625\n",
       "17    1478  300017.156250\n",
       "18    1479  307442.593750\n",
       "19    1480  467588.218750\n",
       "20    1481  314933.968750\n",
       "21    1482  200260.000000\n",
       "22    1483  174575.312500\n",
       "23    1484  163160.312500\n",
       "24    1485  177237.234375\n",
       "25    1486  184873.890625\n",
       "26    1487  342014.187500\n",
       "27    1488  225092.343750\n",
       "28    1489  197313.734375\n",
       "29    1490  240105.265625\n",
       "...    ...            ...\n",
       "1429  2890   90471.804688\n",
       "1430  2891  144831.109375\n",
       "1431  2892   37231.179688\n",
       "1432  2893   73736.148438\n",
       "1433  2894   43988.050781\n",
       "1434  2895  352525.531250\n",
       "1435  2896  286334.875000\n",
       "1436  2897  196819.421875\n",
       "1437  2898  142756.562500\n",
       "1438  2899  204118.843750\n",
       "1439  2900  165214.921875\n",
       "1440  2901  242796.843750\n",
       "1441  2902  170066.656250\n",
       "1442  2903  317440.281250\n",
       "1443  2904  349227.750000\n",
       "1444  2905   56490.425781\n",
       "1445  2906  174882.328125\n",
       "1446  2907  100538.375000\n",
       "1447  2908  134884.640625\n",
       "1448  2909  140087.468750\n",
       "1449  2910   75793.937500\n",
       "1450  2911   79643.992188\n",
       "1451  2912  149441.312500\n",
       "1452  2913   70645.679688\n",
       "1453  2914   73459.820312\n",
       "1454  2915   82787.554688\n",
       "1455  2916   73339.656250\n",
       "1456  2917  165991.234375\n",
       "1457  2918  117034.796875\n",
       "1458  2919  206727.000000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
